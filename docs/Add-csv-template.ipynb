{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for creating a dataset from a single .csv file\n",
    "\n",
    "This example creates a dataset using a single manually downloaded .csv file using a helper function in the `workflow`.\n",
    "\n",
    "The `src` module here should be the name of your project module, whatever you have named it.\n",
    "\n",
    "In this case, we'll use one of the [COVID-19 Open-Data](https://github.com/GoogleCloudPlatform/covid-19-open-data) files from Google: https://storage.googleapis.com/covid19-open-data/v2/epidemiology.csv as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic utility functions\n",
    "import logging\n",
    "import pathlib\n",
    "\n",
    "from src.log import logger\n",
    "from src.data import Dataset\n",
    "from src import paths\n",
    "\n",
    "# data functions\n",
    "from src import workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally set to debug log level\n",
    "#logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reference, this is your current `paths['raw_data_path']` set in your conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths['raw_data_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation information\n",
    "\n",
    "This is the information that you need to provide to create this dataset:\n",
    "\n",
    "* `ds_name`: The name you want to call your dataset in the Dataset catalog\n",
    "* `csv_path`: The desired path to your .csv file (in this case `epidemiology.csv`) relative to paths['raw_data_path']\n",
    "* `download_message`: The message to display to indicate to the user how to manually download your .csv file.\n",
    "* `license_str`: Information on the license for the dataset\n",
    "* `descr_str`: Information on the dataset itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'covid-19-epidemiology'\n",
    "csv_path = 'epidemiology.csv' # path relative to paths['raw_data_path'] for the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_message = f\"\"\"Please retrieve epidemiology.csv from https://storage.googleapis.com/covid19-open-data/v2/epidemiology.csv \\\n",
    "and place it in {paths['raw_data_path']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "license_str = \"\"\"\n",
    "[CC-BY 4.0](https://github.com/GoogleCloudPlatform/covid-19-open-data/blob/main/output/CC-BY)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_str = \"\"\"\n",
    "The epidemiology table from Google's [COVID-19 Open-Data dataset](https://github.com/GoogleCloudPlatform/covid-19-open-data). \n",
    "\n",
    "The full dataset contains datasets of daily time-series data related to COVID-19 for over 20,000 distinct locations around the world. The data is at the spatial resolution of states/provinces for most regions and at county/municipality resolution for many countries such as Argentina, Brazil, Chile, Colombia, Czech Republic, Mexico, Netherlands, Peru, United Kingdom, and USA. All regions are assigned a unique location key, which resolves discrepancies between ISO / NUTS / FIPS codes, etc. The different aggregation levels are:\n",
    "\n",
    "    0: Country\n",
    "    1: Province, state, or local equivalent\n",
    "    2: Municipality, county, or local equivalent\n",
    "    3: Locality which may not follow strict hierarchical order, such as \"city\" or \"nursing homes in X location\"\n",
    "\n",
    "There are multiple types of data:\n",
    "\n",
    "    Outcome data Y(i,t), such as cases, tests, hospitalizations, deaths and recoveries, for region i and time t\n",
    "    Static covariate data X(i), such as population size, health statistics, economic indicators, geographic boundaries\n",
    "    Dynamic covariate data X(i,t), such as mobility, search trends, weather, and government interventions\n",
    "\n",
    "The data is drawn from multiple sources, as listed below, and stored in separate tables as CSV files grouped by context, which can be easily merged due to the use of consistent geographic (and temporal) keys as it is done for the main table.\n",
    "\n",
    "One of these files is the epidemiology.csv file used here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not yet placed your `epidemiology.csv` file in the appropriate place, the following cell will fail with a `FileNotFoundError` to the path it expects for your `epidemiology.csv` file. Put your file in the appropriate place, and then try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = workflow.dataset_from_csv_manual_download(ds_name=ds_name,\n",
    "                                               csv_path=csv_path,\n",
    "                                               download_message=download_message,\n",
    "                                               license_str=license_str,\n",
    "                                               descr_str=descr_str,\n",
    "                                               overwrite_catalog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = Dataset.load(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the workflow helper function also created a `covid-19-epidemiology_raw` dataset that has an empty `ds.data`, but keeps a record of the location of the final `epidemiology.csv` file relative to  in `ds.EXTRA`.\n",
    "\n",
    "The `.EXTRA` functionality is covered in other documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_raw = Dataset.from_catalog(ds_name+\"_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_raw.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw.EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fq path to epidemiology.csv file\n",
    "ds_raw.extra_file('epidemiology.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check-in the catalog\n",
    "The new dataset will now be in the catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.dataset_catalog(keys_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you'll need to check in your new catalog files so that they are shared with others. Then, anyone with the catalog file can `ds.load()` the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:easydata-notebook]",
   "language": "python",
   "name": "conda-env-easydata-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
