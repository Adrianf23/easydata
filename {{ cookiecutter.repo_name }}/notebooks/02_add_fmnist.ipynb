{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding and processing the Fashion-MNIST (FMNIST) Dataset\n",
    "\n",
    "*\"Raw Data is Read Only.\" Sing it with me.*\n",
    "\n",
    "\n",
    "A raw dataset is really just a list of files (and some useful metadata) that is later processed into a usable dataset. From a data provenance perspective, the most important things to know about raw data are:\n",
    "* Raw data is **hash-verified**. This ensures that if something changes upstream, we know about that change.\n",
    "* Raw data is **read only**, and is used to generate a separate and reproducible **Dataset** object\n",
    "* Raw data is **not saved** in the source code repository. (in fact, the whole `data` directory is specifically excluded in our `.gitignore`\n",
    "\n",
    "Our approach to building a usable dataset is:\n",
    "\n",
    "1. Assemble the raw data files.\n",
    "2. Generate (and record) hashes to ensure the validity of these files\n",
    "3. Add LICENSE and DESCR (description) metadata to make the raw data usable for other people, and\n",
    "4. Write a function to process the raw data into a usable format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble the Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"f_mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original MNIST dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "The dataset is free to use under an MIT license.\n",
    "\n",
    "It can be found online at https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Looking at the documentation in this repo, we see that the raw FMNIST is distributed as a set of 4 files. Because Zalando are excellent data citizens, they have conveniently given us MD5 hashes that we can verify for this raw data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Name  | Content | Examples | Size | Link | MD5 Checksum|\n",
    "| --- | --- |--- | --- |--- |--- |\n",
    "| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n",
    "| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n",
    "| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n",
    "| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Directories, `paths` and `pathlib`\n",
    "\n",
    "Recall from our `README.md` the locations of our data files\n",
    "\n",
    "* `data`\n",
    "    * Data directory. often symlinked to a filesystem with lots of space\n",
    "    * `data/raw` \n",
    "        * Raw (immutable) hash-verified downloads\n",
    "    * `data/interim` \n",
    "        * Extracted and interim data representations\n",
    "    * `data/processed` \n",
    "        * The final, canonical data sets for modeling.\n",
    "\n",
    "We don't want to hardcode these paths in our scripts.  This is what the `src.paths` module is for.\n",
    "* `src.paths.raw_data_path`\n",
    "* `src.paths.interim_data_path`\n",
    "* `src.paths.processed_data_path`\n",
    "\n",
    "A quick aside: Use `pathlib`! Read more here:https://realpython.com/python-pathlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import raw_data_path, interim_data_path\n",
    "from src.data.utils import list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../data/raw/*.license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{raw_data_path}\")\n",
    "list_dir(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fetch these files and check their hashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import RawDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_site = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com'\n",
    "file_list = [\n",
    "    ('train-images-idx3-ubyte.gz','8d4fb7e6c68d591d4c3dfef9ec88bf0d'),\n",
    "    ('train-labels-idx1-ubyte.gz','25c81989df183df01b3e8a0aad5dffbe'),\n",
    "    ('t10k-images-idx3-ubyte.gz', 'bef4ecab320f06d8554ea6380940ec79'),\n",
    "    ('t10k-labels-idx1-ubyte.gz', 'bb300cfdad3c16e7a12a480ee83cd310'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = RawDataset(dataset_name)\n",
    "for file, hashval in file_list:\n",
    "    url = f\"{data_site}/{file}\"\n",
    "    fmnist.add_url(url=url, hash_type='md5', hash_value=hashval)\n",
    "fmnist.add_url(url='https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/LICENSE',\n",
    "            name='LICENSE', file_name='f_mnist.license')\n",
    "fmnist.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.fetched_files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_opts = fmnist.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(**ds_opts); print(ds.LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.utils import partial_call_signature\n",
    "partial_call_signature(fmnist.load_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp.file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load the newly created dataset\n",
    "ds = load_dataset('f_mnist')\n",
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a `Dataset` object?\n",
    "* data: the processed data\n",
    "* target: (optional) target vector (for supervised learning problems)\n",
    "* metadata: Data about the data\n",
    "\n",
    "Under the hood, this is basically a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, this is empty, since we haven't given any indication how to process the raw files into usable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the raw files into `data` and `target`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we turn these raw files into processed data?\n",
    "\n",
    "First, we need to unpack them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import fetch_and_unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untar_dir = fetch_and_unpack(dataset_name)\n",
    "print(f\"{untar_dir}:\\n {list_dir(untar_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch_and_unpack knows how to handle most compressed data types. Unpacked data is stored at `interim_data_path/dataset_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.paths import interim_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(interim_data_path / dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
    "we see how to process this data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need numpy. How do we add this to the environment?\n",
    "* Add it to `environment.yml`\n",
    "* `make requirements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = \"train\"\n",
    "label_path = interim_data_path / dataset_name / f\"{kind}-labels-idx1-ubyte\"\n",
    "with open(label_path, 'rb') as fd:\n",
    "    target = np.frombuffer(fd.read(), dtype=np.uint8, offset=8)\n",
    "dataset_path = interim_data_path / dataset_name / f\"{kind}-images-idx3-ubyte\"\n",
    "with open(dataset_path, 'rb') as fd:\n",
    "    data = np.frombuffer(fd.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is this data? We should document what this Dataset represents. Enter the first of two special pieces of metadata: DESCR. Let's be nice to our users and document out dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_readme = '''\n",
    "Fashion-MNIST\n",
    "=============\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 70000\n",
    "    :Number of Attributes: 728\n",
    "    :Attribute Information: 28x28 8-bit greyscale image\n",
    "    :Missing Attribute Values: None\n",
    "    :Creator: Zalando\n",
    "    :Date: 2017\n",
    "\n",
    "This is a copy of Zalando's Fashion-MNIST [F-MNIST] dataset:\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a\n",
    "training set of 60,000 examples and a test set of 10,000\n",
    "examples. Each example is a 28x28 grayscale image, associated with a\n",
    "label from 10 classes. Fashion-MNIST is intended to serve as a direct\n",
    "drop-in replacement for the original MNIST dataset for benchmarking\n",
    "machine learning algorithms. It shares the same image size and\n",
    "structure of training and testing splits.\n",
    "\n",
    "References\n",
    "----------\n",
    "  - [F-MNIST] Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\n",
    "    Han Xiao, Kashif Rasul, Roland Vollgraf. arXiv:1708.07747\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import add_dataset_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dataset_metadata(dataset_name, kind='DESCR', from_str=fmnist_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyc_easydata]",
   "language": "python",
   "name": "conda-env-nyc_easydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
